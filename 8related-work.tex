\section{Related Work and Novelty of Our Approach}
\label{sec:related-work}

Besides behaviorally synthesized pipelines, there are mainly two other kinds of pipelines, hardware pipelines and software pipelines. 
%There has been a significant amount of work in verification of hardware pipelines
%There are significant differences in goals
%and techniques between these efforts and ours.
Microprocessor pipelines~\cite{pvs,Cyrluk94, bd:pipeline, sh:pipeline} include optimized (hand-crafted)
control and forwarding logics, but have a static set of
operations based on the instruction set. Behaviorally synthesized 
loop pipelines tend to be deep with a high complexity at each stage, but
control and forwarding logics are more standardized since
they are automatically synthesized. 
Furthermore, microprocessor pipeline verification
is focused on one (hand-crafted) pipeline implementation,
while our work focuses on verifying an {\em algorithm
that generates pipelines}. Our invariant is very different from a typical invariant
used in the verification of pipelined machines~\cite{sh:pipeline} . We make explicit the
correspondence with the sequential execution. The key
requirement from a pipeline invariant, \viz, hazard freedom,
is left implicit and arises indirectly as a proof obligation
for invariance of this predicate. 
 
%Behaviorally synthesized loop pipelines are similar in reasoning to software loop pipelines except that since
%behavioral synthesis is automatic, it is much more streamlined than software pipelines. 
Our understanding of hazards and reasoning behind pipelining algorithm 
is very closely related to verification of software pipelines.  In particular, Tristan
and Leroy~\cite{tl:software-popl10} present a verified
translation validator for software loop pipelines.  The loop
pipelines in behavioral synthesis considered in this paper
are close in structure to software loop pipelines, although
our formalization (\eg, CCDFG) has different semantics from
the Control Flow Graphs they use, reflecting the difference
between eventual targets of compilation (\viz, hardware
vs. software).  However, the fundamental difference is in
the approach taken to actually certify the pipelines.
Tristan and Leroy's approach decomposes the certification
problem into two parts, a ``dynamic'' part that is certified
on a case-by-case basis  and a ``static'' part that is
certified in the Coq theorem prover~\cite{coq} once and for all.  The
theorem proven by Coq is informally paraphrased as follows:

\begin{quote}
Suppose the pipelining algorithm generates a pipeline
${\cal{P}}$ from a sequential design ${\cal{S}}$.  Suppose
symbolic simulation of ${\cal{S}}$ and ${\cal{P}}$ verifies
certain ``dynamic'' verification conditions (VCs).  Then
${\cal{S}}$ and ${\cal{P}}$ are indeed semantically
equivalent.
\end{quote}

\noindent
Thus for any pipeline instance ${\cal{P}}$ generated by
their algorithm, symbolic simulation is executed between
${\cal{P}}$ and ${\cal{S}}$ to certify that ${\cal{P}}$ is
indeed a correct pipelined implementation of ${\cal{S}}$.
The dynamic VCs checked by symbolic simulation essentially
certify that the pipeline generation did not overlook any
hazards.

This is where our work differs from theirs.  Our work is
expected to provide a single theorem certifying the
correctness of the reference pipelined implementation,
without requiring further runtime hazard check.
Furthermore, their correspondence theorem relates the
pipelined implementation with a sequential design with a
(bounded) unrolled loop, while our approach certifies the
correspondence between the actual Control Flow Graph (CFG)
and the pipelined implementation.  Indeed, Tristan and Leroy
remark that the mechanization of the correspondence between
the CFG and unrolled loop is ``infuriatingly difficult''.
We speculate this is so because they focus on verifying the
correspondence between the unrolled loop and the pipeline.
In our experience, attempting the formal correspondence between the unrolled
sequential loop and pipelined design is indeed difficult since
there is no formal way to connect to back edge of the loop
with any of the edges in the pipeline.  We believe that
reconciling this problem and developing a fully certified
pipeline generation algorithm would require backtracking
from the correspondence with an unrolled loop (and hence
translation validation) to a more complex invariant like
ours.  Of course we must note that we can ``afford'' to
develop a fully certified algorithm in our approach since
the pipelines are simpler
(cf. Chapter~\ref{sec:formalization}); achieving this for
arbitrary software pipeline may require further more subtle
invariants.

\begin{comment}
%Thorem provers are widely used for hardware verification. 
%HOL theorem prover~\cite{hol} has been used in 
%several well-documented projects~\cite{cohn,graham}.
ACL2 is also used a lot in hardware 
verification~\cite{Russinoff,car2,car,Hardin,ray-abstracting,ray-connecting,ray-certification}. 
Our project is however somewhat different from the
traditional applications of theorem provers. 
First, since an over-arching goal is to exploit automatic
decision procedures, we use theorem proving primarily to
complement automated tools. Second, we eschew theorem
proving on inherently complex or low-level implementations.
Third, interactive theorem proving is acceptable for
one-time use, in certification of a transformation, 
but not as part of a methodology that
requires ongoing use in certification of each design. The
constraints are imposed by the the environment in which we
envision our framework being deployed: it may not be
possible to have a dedicated team of experts doing theorem
proving as full-time jobs. Finally, the loop pipelining transformation
we verify are proprietary to the synthesis tools. Therefore, our approach is
targeting verification of transformations
which are closed-source (and exceedingly complex), thus
making traditional program verification techniques unusable.
Our approach shows a novel way in which theorem
proving can be applied even under those constraints, in
concert with automated SEC.

In addition to technical contributions, we see our work as
providing an important methodological contribution enabling
use of theorem proving in situations where one needs to
certify the result of an implementation on which theorem
proving cannot be directly applied either because it is
closed-source or because it is highly complex: (1)~create a
reference implementation, perhaps using as much information
as available from the actual implementation, in our case
information about pipeline intervals, (2)~certify this
simpler reference implementation with theorem proving, and
(3)~develop an SEC framework to compare the result of the
reference implementation with that of the actual
implementation.  In addition to making theorem proving
applicable on industrial flows without requiring us to
certify industial implementations with their full
complexity, this approach permits adjusting the algorithm
(within limits) to suit mechanical reasoning while still
affording comparison with actual synthesized artifacts.  We
have made liberal use of this ``luxury'', \eg, we have
been continually redefining our superstep construction function
to facilitate proof of key structural lemmas of the
invariant before settling on the final version.  We believe similar approach is applicable in
other contexts and may provide effective use of theorem
proving within industrial verification flows.
\end{comment}