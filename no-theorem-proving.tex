\section{Challenges}
\label{sec:not-verifiable}

Verifying the correspondence between a sequential design and
its pipelined RTL by traditional verification methods is
challenging.  Loop pipelining is a complex transformation
which introduces temporal overlap of iterations. As a
result, the industrial synthesis tools employ the following
features:

\begin{enumerate}
\item Aggressive scheduling strategies are used to produce a
  new schedule.
\item Order of executions of basic block is different from
that of the sequential design, thus changing control flow.
\item New variables are introduced to get rid of data hazards.
\end{enumerate}

Consequently, mappings between the internal operations of
sequential design and pipelined RTL are destroyed. This renders SEC ineffective because in the absence of mappings, SEC gets reduced to checking
input and output correspondence of designs.
This leads to
state space explosion in large designs. Furthermore, since
the synthesis tool vendor typically does not disclose
the implementation of the transformation, direct
certification of the transformation by theorem proving is
also not possible.

However, one key observation is that it is {\em not necessary} to verify the exact transformation implemented
by the behavioral synthesis tool in order to certify the
correctness of the synthesized pipelined RTL. In
particular, one can create one's own pipelining algorithm, that takes a sequential CCDFG and creates the
pipelined counterpart; we can use the latter as a {\em reference
  model} to perform SEC against the synthesized RTL.
Furthermore, the pipelining algorithm can use the
reports from the behavioral synthesis tool (\eg, values of
pipeline parameters, number of iterations pipelined, etc.)
as guidance in generating the pipeline. If such a
pipelining algorithm can be developed and certified
by theorem proving, then we will have exactly the same
confidence in synthesized RTL as if we had certified the
pipelining algorithm used by behavioral
synthesis.\footnote{Of course the algorithm generating the
  reference model would be simpler, and may fail to generate a pipeline when the synthesis tool can synthesize one using more advanced heuristics.  However, {\em if we can generate a pipelined CCDFG which can be used for SEC with the synthesized RTL}, then that is sufficient for
  certification.}

Hao {\em et al.}'s algorithm
(cf. Section~\ref{subsec:kecheng's algorithm}) is a step in
that direction. In particular, they make use of certain
parameter values gleaned from the reports generated by
behavioral synthesis tool to develop a pipelined reference
model which they could use for SEC against the synthesized
RTL. Nevertheless, their algorithm falls short of the
requirement for a {\em certifiable} algorithm.  In
particular, since their algorithm is not designed with
reasoning in mind, it is non-trivial to certify it as it is.

To understand the complexities involved in mechanical
certification of an algorithm that was not designed
originally with certification in mind, we need to re-visit
the general approach to applying formal reasoning on
software programs.  The typical approach is to break the
program into a number of pieces, prove key lemmas
characterizing the role of each piece, and then chain these
lemmas together into a proof of the correctness of the
entire program. Crucial to this approach, however, is the
requirement that each program piece can be characterized by
a succinct invariant that can be easily verified.  However,
in a program not developed with reasoning in mind,
optimizations typically destroy the structural disciplines
and modularity of the individual program pieces. This makes it
difficult to identify and isolate the components that
actually maintain succinct, interesting invariants.  The
result is that in order to certify such an implementation,
one has to either (1)~restructure the implementation into
one that is more disciplined, and prove the equivalence
between the two, or (2)~come up with very complex
invariants that essentially comprehend how invariants from
each individual piece are conflated together in the
implementation.  Both approaches require extensive human
interaction, resulting in the proverbial euphemism of proofs
of programs being orders of magnitude more complex than the
programs themselves~\cite{liu}.

%% It is common knowledge that certifying a complex algorithm
%% which is not written with reasonis
%% difficult to certify. To certify an algorithm, we divide an
%% algorithm into components such that we can identify
%% predicates which are maintained by various components of the
%% algorithm (property of variables of the algorithm). The
%% complexity of verifying an algorithm comes from the
%% complexity of defining and proving these predicates.  We can
%% choose a simple invariant --- state after completely
%% executing the CCDFG is same before and after the various
%% components of the algorithm. But, since the algorithm has
%% not been written keeping this invariant in mind, the various
%% components of the proposed algorithm do not follow this
%% invariant even though the complete algorithm has this
%% property. The components conflate and it is difficult to
%% entangle them in this case.

%% We can come up with a complex invariant to certify the algorithm. There are approaches to do it such as on-track property. But, it is highly complex. (\hl{A small program has costed people PhD (cite examples)}).

In our work, however, we can ``get away'' without verifying
the specific implementation while still being able to
certify the design generated by behavioral synthesis without
loss of fidelity. The key observation, as above, is that it
is sufficient to develop {\em any} certifiable algorithm
that generates a pipelined CCDFG from a sequential
implementation which can be effectively applied with SEC.
In particular, any certifiable algorithm that has the same
input-output characteristic as Hao {\em et al}.'s algorithm
is sufficient.  Thus, this research focuses on identifying
certifiable primitives and invariants of a loop pipelining
transformation and developing a pipeline generation
algorithm using those primitives, achieving the dual goal of
mechanical reasoning of the algorithm and amenability of the
resulting reference model to SEC.


%% Typical theorem proving does not have flexibility of
%% defining an algorithm. However, in our case, we do not
%% need to certify the same algorithm. We just need a
%% certified algorithm which creates a pipeline reference
%% model structurally similar to pipelined RTL. So, we can
%% design our own certifiable pipeline algorithm which has a
%% natural inclination to be certified by theorem proving.

%Certifying an arbitrary algorithm by theorem proving is not always easy.

%To certify an algorithm by theorem proving,an obvious approach is to identify pre-conditions, post-conditions and the invariants i.e, properties that must remain true for the design before and after the algorithm. 

%In our case, the invariant is obvious from our definition of correctness of the pipelined design (refer section ~\ref{subsec:correctness-defn}). We want that if we begin with the same state, the value of all variables in the state (except local variables in the loop introduced as part of pipeline registers) remains same after completely executing the two designs at run time i.e, for all possible combinations of inputs and paths, the two designs (sequential and pipelined implementations) give the same state after execution.

%The algorithm described above is not designed keeping certification in mind. It does not follow the invariant at any step. Infact, even till the third step of the algorithm, the proposed pipelined design still has some data conflicts which are taken care of by data propogation. So, we do not get an advantage of building our proof on steps in this algorithm. Only after completing all the steps we can claim correct execution.

%So, we attempted to prove this invariant on the whole algorithm, which was the only choice left. Indeed, our original proof attempt was different: we tracked for each variable $v$ the microsteps where it is accessed, and proved that $v$ is read and written in succession on ``corresponding'' microsteps in the sequential and pipelined CCDFGs.  However, this lemma cannot be directly used in the final theorem: different variables are accessed at different microsteps; hence, it is difficult to prove a general property about {\em all} variables by induction on the execution steps.

%A second approach to prove the entire algorithm is to come up with an "ontrack property". We assume that when the algorithm terminates, the invariants hold, otherwise we are on-track to prove the invariants. However, defining and proving an on-track property is not an easy task. (cite examples - anthony fox, hanbing liu, magnus myreen).

%So, certifying an adhoc algorithm is theorem proving can be complex. Typical theorem proving does not have flexibility of defining an algorithm. However, in our case, we have a non-standard application of theorem proving. We do not need to certify the same algorithm. We just need a certified algorithm which follows the criterias described before. So, we can design a pipelining algorithm which has a natural inclination to be certified by theorem proving. 

%We realized that at the crux of a pipeline algorithms in behavioral synthesis, there are three primitives that have the capability of affecting the execution of a pipeline. If we can certify these three primitives to be correct by theorem proving, we can introduce some small tweaks to restructure the proposed algorithm so that each step of the algorithm is a combination of these three primitives. Also, it would allow us to extend this framework to create other certified pipelining algorithms if needed.    
